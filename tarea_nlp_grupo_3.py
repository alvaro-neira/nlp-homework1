# -*- coding: utf-8 -*-
"""Tarea_NLP_Grupo_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Glkt-xfEtkchsRtNVaeZ47h4QCY8WaM

#Integrantes de la tarea

Integrantes del grupo:

* Integrante 1:Matias Rodriguez U. Rut: 18362815-1
* Integrante 2:Paula Canales G. Rut:18845155-1
* Integrante 3:Sebastián Alday Rut:18294095-k
* Integrante 4:Álvaro Neira R. Rut:13757209-5

# Importación de Librerias

Estas son las bibliotecas y módulos necesarios.
"""

import pandas as pd
import re

import spacy
import nltk
import sklearn
import wordcloud
import matplotlib.pyplot as plt
import pandas as pd
import random
import numpy as np
import json
from sklearn.model_selection import train_test_split

"""# Datos Necesarios

Descargamos los datos necesarios para las actividades
"""

!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/aysen.csv
#!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/aysen_corto.csv
#!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/proveedoresA.txt
#!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/other_vhosts_access.log
nltk.download('stopwords')
nltk.download('wordnet')
!python -m spacy download es_core_news_sm
import es_core_news_sm
nlp_es = es_core_news_sm.load()

"""# 1 Limpieza de la base de datos

##Dataset
"""

aysen = pd.read_csv("aysen.csv", sep=";", na_values=["Otro",""])

aysen.head()

"""##1.1 Creación del DK10

###Especialidades con más de 10.000 derivaciones
"""

D10K_ranking=pd.DataFrame(aysen[["SOSPECHA_DIAG","PRESTA_EST"]].PRESTA_EST.value_counts(dropna=False))
D10K_ranking=D10K_ranking.loc[D10K_ranking["PRESTA_EST"]>10000]
D10K_ranking

list(D10K_ranking.index)

"""###Definición DK10

Se necesita de un ciclo para recuperar la columna "SOSPECHA_DIAG" a partir de las especialidades seleccionadas por el rankin
"""

z1=0
z2=0
presta=[]
sospecha=[]
for u in aysen["PRESTA_EST"]:
  for v in list(D10K_ranking.index):
    if u==v:
      #print(u)
      presta.append(u)
      sospecha.append(aysen["SOSPECHA_DIAG"].iloc[z1])
    z2=z2+1 
  z1=z1+1

"""Se concatenan y se crea DK10"""

DK10=pd.concat([pd.DataFrame(sospecha).rename(columns={0:"SOSPECHA_DIAG"}),pd.DataFrame(presta).rename(columns={0:"PRESTA_EST"})],axis=1)
DK10=DK10.reset_index(drop=True)

DK10

"""##1.2 Transformación del texto con patrones regulares"""

import re

# Corrígeme
# pattern = r"""(?x)                   # set flag to allow verbose regexps
#               (?:[A-Z]\.)+           # abbreviations, e.g. U.S.A.
#               |\d+(?:\.\d+)?%?       # numbers, incl. currency and percentages
#               |\w+(?:[-']\w+)*       # words w/ optional internal hyphens/apostrophe
#               |(?:[+/\-@&*])         # special characters with meanings
#             """

pattern = r"""(?x)                   # set flag to allow verbose regexps
              (?:[A-Z]\.)+           # abbreviations, e.g. U.S.A.
              |\$?\d+(?:[.,]\d+)?%?  # numbers, incl. currency and percentages
              |\w+(?:[-']\w+)*       # words w/ optional internal hyphens/apostrophe
              |(?:[+/\-@&*¡!])     # special characters with meanings
            """
pattern2 = r"""                 # set flag to allow verbose regexps
              (\s)([A-Z])         # abbreviations, e.g. U.S.A.

            """
replace= r"""$1\l$2"""
def cambiate(text):
  hola=nltk.regexp_tokenize(text, pattern)
  holanuevo=[]
  for word in hola:
      holanuevo.append(re.sub(r'([A-Z])', lambda pat: pat.group(1).lower(), word) )
  separator = ' '
  hola2=separator.join(holanuevo)
  return hola2

cambiate(DK10["SOSPECHA_DIAG"].iloc[121285])

"""Se hace un experimento, el programa debería quitar la ",""""

DK10["SOSPECHA_DIAG"].iloc[121285]="HIPERTENSION ESENCIAL, PRIMARIA"

print(DK10["SOSPECHA_DIAG"].iloc[121285])

DK10['SOSPECHA_DIAG_TRANSFORMADO'] = DK10.apply(lambda x: cambiate(x.SOSPECHA_DIAG), axis=1)

DK10

"""##1.3 Transformación del texto quitando los stopwords"""

nltk_stopwords = nltk.corpus.stopwords.words('spanish')
spacy_stopwords = spacy.lang.es.stop_words.STOP_WORDS

stop_words_union = set(list(nltk_stopwords) + list(spacy_stopwords))
stop_words_final = []
for word in stop_words_union:
  if word not in spacy_stopwords:
    stop_words_final.append(word)
  if word not in nltk_stopwords:
    stop_words_final.append(word)


def cambiate2(text):
  hola=nltk.regexp_tokenize(text, pattern)
  nuevohola=[]
  for word in hola:
    if word not in stop_words_final:
      nuevohola.append(word)
  separator = ' '
  hola2=separator.join(nuevohola)
  return hola2

len(stop_words_final)

"""Se hace un experimento colocando un stopword, en este caso "cerca""""

DK10["SOSPECHA_DIAG"].iloc[121285]="hipertension esencial primaria cerca"

print(DK10["SOSPECHA_DIAG"].iloc[121285])

"""Después de ocupar la función, efectivamente se quita esta palabra"""

print(cambiate2(DK10["SOSPECHA_DIAG"].iloc[121285]))

"""Se hace la transformación completa."""

DK10['SOSPECHA_DIAG_TRANSFORMADO'] = DK10.apply(lambda x: cambiate2(x.SOSPECHA_DIAG_TRANSFORMADO), axis=1)

"""Se puede apreciar que se elimina la stopword, como también se puede concluir que se deberían quitar las demás en el caso de que las haya."""

DK10

"""##1.4 Transformación del texto con stemmer"""

stemmer = nltk.stem.SnowballStemmer("spanish")
#lemmatizer = nltk.stem.WordNetLemmatizer() no hay en español

def cambiate3(text):
  hola=nltk.regexp_tokenize(text, pattern)
  nuevohola=[]
  for word in hola:
    nuevohola.append(stemmer.stem(word))
  separator = ' '
  hola2=separator.join(nuevohola)
  return hola2

stemmer.stem("especificada")

DK10['SOSPECHA_DIAG_TRANSFORMADO'] = DK10.apply(lambda x: cambiate3(x.SOSPECHA_DIAG_TRANSFORMADO), axis=1)

DK10

"""##DK10 Crudo y Procesado"""

DK10_crudo=DK10[["SOSPECHA_DIAG","PRESTA_EST"]].copy()
DK10_crudo

DK10_procesado=DK10[["SOSPECHA_DIAG_TRANSFORMADO","PRESTA_EST"]].copy()
DK10_procesado=DK10_procesado.rename(columns={"SOSPECHA_DIAG_TRANSFORMADO":"SOSPECHA_DIAG"})
DK10_procesado

"""#  2 Análisis no-supervisado y visualización

* Utilice en esta sección la salida de la sección de limpieza, o el texto crudo.

### 2.1 Visualice utilizando wordcloud las palabras más comunes para cada categoría D10K.

* En esta parte me baso en **2-normalization_and_vector_semantics.sol.ipynb**

* En esta seccion utilizamos los stopwords calculados en el punto 1.3
"""

stopwords_aysen = stop_words_union

def plot_wordcloud_from_specialty(dataset, specialty):
  tfidf_dict = dict(zip(tfidf_vectorizer_aysen.get_feature_names_out(),x_aysen.toarray()[dataset.PRESTA_EST == specialty].mean(0).reshape(-1,)))
  tfidf_dict = {word:val for word,val in tfidf_dict.items() if val > 0}
  wc = wordcloud.WordCloud(width=1600, height=800).generate_from_frequencies(tfidf_dict)
  plt.figure(figsize=(10,5))
  plt.imshow(wc)
  plt.axis("off")
  plt.show()

tfidf_vectorizer_aysen = sklearn.feature_extraction.text.TfidfVectorizer(stop_words=stopwords_aysen,max_df=0.20)
x_aysen = tfidf_vectorizer_aysen.fit_transform(DK10_procesado.SOSPECHA_DIAG)

tfidf_vectorizer_raw = sklearn.feature_extraction.text.TfidfVectorizer(stop_words=stopwords_aysen,max_df=0.20)
x_raw = tfidf_vectorizer_raw.fit_transform(DK10_crudo.SOSPECHA_DIAG)
def plot_wordcloud_from_specialty_raw(dataset, specialty):
  tfidf_dict = dict(zip(tfidf_vectorizer_raw.get_feature_names_out(),x_raw.toarray()[dataset.PRESTA_EST == specialty].mean(0).reshape(-1,)))
  tfidf_dict = {word:val for word,val in tfidf_dict.items() if val > 0}
  wc = wordcloud.WordCloud(width=1600, height=800).generate_from_frequencies(tfidf_dict)
  plt.figure(figsize=(10,5))
  plt.imshow(wc)
  plt.axis("off")
  plt.show()

for specialty in list(D10K_ranking.index):
  print(f"DK10_procesado: {specialty}")
  plot_wordcloud_from_specialty(DK10_procesado, specialty)
  print(f"")
  print(f"DK10_crudo: {specialty}")
  plot_wordcloud_from_specialty(DK10_crudo, specialty)
  print(f"")

"""# 3. Análisis supervisado (2 pts)

Utilice en esta sección la salida de la sección de limpieza, o el texto crudo.

**En esta parte volví a calcular el D10K porque creo que así es mejor**
"""

aysen_data = aysen[["SOSPECHA_DIAG","PRESTA_EST"]]

rank = json.loads(aysen_data.PRESTA_EST.value_counts().to_json())
rank = pd.DataFrame(list(rank.items()),columns = ['PRESTA_EST','PRESTA_EST_COUNT'])
rank = rank.loc[rank['PRESTA_EST_COUNT'] > 10000]
rank

rank_list = rank.PRESTA_EST.values.tolist()
rank_list

D10K = aysen_data.loc[aysen_data['PRESTA_EST'].isin(rank_list)]
D10K

"""## 3.1 Probabilidad de cada categoría en D10K"""

total = rank.PRESTA_EST_COUNT.sum()
rank['PROB'] = rank.PRESTA_EST_COUNT/total
rank

"""## 3.2 Diccionario P(palabra|categoria)."""

train_aysen, test_aysen = train_test_split(D10K, test_size=0.1)

count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(stop_words=stopwords_aysen)
term_doc_matrix_train = count_vectorizer.fit_transform(train_aysen.SOSPECHA_DIAG)
term_doc_matrix_train

p_word_category = {}
for index, row in rank.iterrows():
  category = row['PRESTA_EST']
  count_c = row['PRESTA_EST_COUNT']
  # Se obtienen las apariciones de cada una de las palabras en cada clase
  c_w = np.asarray(term_doc_matrix_train.todense()[train_aysen.PRESTA_EST == category,:].sum(0)).reshape(-1)
  # Se calcula la probabilidad
  p_w = (c_w + 1) / count_c
  p_word_category[category] = p_w
p_word_category

"""## 3.3 Construya un clasificador Bayesiano Naive. Reporte el resultado de 3 métricas de clasificación ¿Detecto alguna categoría de mayor dificultad? ¿puede sugerir alguna interpretación a su resultado?

Test inicial: clase positiva "es traumatologia", negativa = "no es traumatologia"
"""

# train_aysen = train
# test_aysen = test

train_aysen.PRESTA_EST

"""Nuestra matriz de términos y documentos tiene la forma (documentos,términos)"""

# term_doc_matrix_train se construyo en el 3.2
term_doc_matrix_train

term_doc_matrix_train.shape

"""Calculamos la cantidad de apariciones de cada una de las palabras en cada una de las clases, junto con la cantidad de apariciones de cada clase."""

#c_w_negative = np.asarray(term_doc_matrix_train.todense()[dataset.PRESTA_EST != "TRAUMATOLOGIA",:].sum(0)).reshape(-1)
#term_doc_matrix_train.todense()
#[dataset.PRESTA_EST != "TRAUMATOLOGIA",:]

#number of negative WORDS (array)
# c_w_negative = np.asarray(term_doc_matrix_train.todense()[dataset.PRESTA_EST != "TRAUMATOLOGIA",:].sum(0)).reshape(-1)
# c_w_negative = np.asarray(term_doc_matrix_train.todense()[imdb[imdb.is_valid == False].label == "negative",:].sum(0)).reshape(-1)
c_w_negative = np.asarray(term_doc_matrix_train.todense()[train_aysen.PRESTA_EST != "TRAUMATOLOGIA",:].sum(0)).reshape(-1)
#number of positive WORDS (array)
c_w_positive = np.asarray(term_doc_matrix_train.todense()[train_aysen.PRESTA_EST == "TRAUMATOLOGIA",:].sum(0)).reshape(-1)
#a number
c_negative = (train_aysen.PRESTA_EST != "TRAUMATOLOGIA").sum() # apariciones de la clase negativa
#a number
c_positive = (train_aysen.PRESTA_EST == "TRAUMATOLOGIA").sum()

print(f"c_w_negative={c_w_negative}")
print(f"c_w_positive={c_w_positive}")
print(f"c_negative={c_negative}\nc_positive={c_positive}")

"""Ratio positivo - negativo.

Calcule el ratio positivo negativo en esta base de datos, y muestre cuales son las 3 palabras más probables en cada clase.
"""

p_w_negative = (c_w_negative + 1) / c_negative # Probabilidad asociada a cada palabra en la clase negativa
p_w_positive = (c_w_positive + 1) / c_positive

# Programa
r = np.log(p_w_positive / p_w_negative)
r

"""### Clasificador"""

# Aca se deberia usar el conjunto de VALIDACION
term_doc_matrix_test = count_vectorizer.transform(test_aysen.SOSPECHA_DIAG)
test_pred = term_doc_matrix_test.dot(r) > 0
test_label = np.where(test_aysen.PRESTA_EST == "TRAUMATOLOGIA", True, False)
confusion_matrix = pd.crosstab(test_label,test_pred)
print(np.array(confusion_matrix).diagonal().sum() / np.array(confusion_matrix).sum())

def binary_naive_bayes(specialty):
  c_w_negative = np.asarray(term_doc_matrix_train.todense()[train_aysen.PRESTA_EST != specialty,:].sum(0)).reshape(-1)
  c_w_positive = np.asarray(term_doc_matrix_train.todense()[train_aysen.PRESTA_EST == specialty,:].sum(0)).reshape(-1)
  c_negative = (train_aysen.PRESTA_EST != specialty).sum()
  c_positive = (train_aysen.PRESTA_EST == specialty).sum()
  p_w_negative = (c_w_negative + 1) / c_negative # Probabilidad asociada a cada palabra en la clase negativa
  p_w_positive = (c_w_positive + 1) / c_positive 
  r = np.log(p_w_positive / p_w_negative)
  term_doc_matrix_test = count_vectorizer.transform(test_aysen.SOSPECHA_DIAG)
  test_pred = term_doc_matrix_test.dot(r) > 0
  test_label = np.where(test_aysen.PRESTA_EST == specialty, True, False)
  confusion_matrix = pd.crosstab(test_label,test_pred)
  return (np.array(confusion_matrix).diagonal().sum() / np.array(confusion_matrix).sum())

print(f"{binary_naive_bayes('TRAUMATOLOGIA')}")

for specialty in rank_list:
  print(f"specialty: {specialty}: {binary_naive_bayes(specialty)}")

"""## 3.4 ¿Cómo varía su resultado al utilizar bigramas?"""

# Programa
count_vectorizer_bigrams = sklearn.feature_extraction.text.CountVectorizer(stop_words=stopwords_aysen,ngram_range=(1,2))
term_doc_matrix_train_bigrams = count_vectorizer_bigrams.fit_transform(train_aysen.SOSPECHA_DIAG)
c_w_negative_bigrams = np.asarray(term_doc_matrix_train_bigrams.todense()[train_aysen.PRESTA_EST != "TRAUMATOLOGIA",:].sum(0)).reshape(-1) # apariciones de cada una de las palabras en la clase negativa
c_w_positive_bigrams = np.asarray(term_doc_matrix_train_bigrams.todense()[train_aysen.PRESTA_EST == "TRAUMATOLOGIA",:].sum(0)).reshape(-1)
p_w_negative_bigrams = (c_w_negative_bigrams + 1) / c_negative # Probabilidad asociada a cada palabra en la clase negativa
p_w_positive_bigrams = (c_w_positive_bigrams + 1) / c_positive 
r_bigrams = np.log(p_w_positive_bigrams / p_w_negative_bigrams)
term_doc_matrix_test_bigrams = count_vectorizer_bigrams.transform(test_aysen.SOSPECHA_DIAG)
test_pred_bigrams = term_doc_matrix_test_bigrams.dot(r_bigrams) > 0
confusion_matrix_bigrams = pd.crosstab(test_label,test_pred_bigrams)
print(np.array(confusion_matrix_bigrams).diagonal().sum() / np.array(confusion_matrix_bigrams).sum())