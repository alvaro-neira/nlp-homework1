# -*- coding: utf-8 -*-
"""Tarea_NLP_Grupo_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ygJ1913q3Z5l2U-CPdIH8BYIUiQ6Dv6B

# Introducción

Estas son las bibliotecas y módulos necesarios.
"""

import pandas as pd
import re

import spacy
import nltk
import sklearn
import wordcloud
import matplotlib.pyplot as plt
import pandas as pd

"""Descargamos los datos necesarios para las actividades"""

# !wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/aysen.csv
# !wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/aysen_corto.csv
# !wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/proveedoresA.txt
# !wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/other_vhosts_access.log
nltk.download('stopwords')
nltk.download('wordnet') 
# !python -m spacy download es_core_news_sm
# import es_core_news_sm
# nlp_es = es_core_news_sm.load()
nlp_es = spacy.load('es_core_news_sm')

"""## Actividad 1: Lectura y parsing

1.   Utilizando la biblioteca Pandas, cargue el archivo `aysen_corto.csv` como un dataframe (revise los tipos de datos, cabecera y separador)
2.   Mantenga solo las columnas `sexo`, `comuna`, y `prestación`.
3.   Realice un conteo de los valores en de las columnas `sexo` y `comuna` y preséntelos en un gráfico.


"""

# HINTS:
# https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html
# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html

# Prográmame

aysen = pd.read_csv("aysen.csv", sep=";", na_values=["Otro",""])

aysen.head()

aysen_subset = aysen[["SOSPECHA_DIAG","PRESTA_EST"]]

aysen_subset.head()

aysen_subset.PRESTA_EST.value_counts(dropna=False).plot.pie()

D10K=pd.DataFrame(aysen_subset.PRESTA_EST.value_counts(dropna=False))
D10K=D10K.loc[D10K["PRESTA_EST"]>10000]
D10K

list(D10K.index)

z1=0
z2=0
presta=[]
sospecha=[]
for u in aysen["PRESTA_EST"]:
  for v in list(D10K.index):
    if u==v:
      #print(u)
      presta.append(u)
      sospecha.append(aysen["SOSPECHA_DIAG"].iloc[z1])
    z2=z2+1 
  z1=z1+1

aysen_nuevo=pd.concat([pd.DataFrame(sospecha).rename(columns={0:"SOSPECHA_DIAG"}),pd.DataFrame(presta).rename(columns={0:"PRESTA_EST"})],axis=1)
aysen_nuevo=aysen_nuevo.reset_index(drop=True)

aysen_nuevo

# Corrígeme
# pattern = r"""(?x)                   # set flag to allow verbose regexps
#               (?:[A-Z]\.)+           # abbreviations, e.g. U.S.A.
#               |\d+(?:\.\d+)?%?       # numbers, incl. currency and percentages
#               |\w+(?:[-']\w+)*       # words w/ optional internal hyphens/apostrophe
#               |(?:[+/\-@&*])         # special characters with meanings
#             """

pattern = r"""(?x)                   # set flag to allow verbose regexps
              (?:[A-Z]\.)+           # abbreviations, e.g. U.S.A.
              |\$?\d+(?:[.,]\d+)?%?  # numbers, incl. currency and percentages
              |\w+(?:[-']\w+)*       # words w/ optional internal hyphens/apostrophe
              |(?:[+/\-@&*¡!.,])     # special characters with meanings
            """

def cambiate(text):
  hola=nltk.regexp_tokenize(text, pattern)
  separator = ' '
  hola2=separator.join(hola)
  return hola2

cambiate(aysen_nuevo["SOSPECHA_DIAG"].iloc[121285])

aysen_nuevo['SOSPECHA_DIAG_NUEVO'] = aysen_nuevo.apply(lambda x: cambiate(x.SOSPECHA_DIAG), axis=1)

aysen_nuevo

nltk_stopwords = nltk.corpus.stopwords.words('spanish')
spacy_stopwords = spacy.lang.es.stop_words.STOP_WORDS

stop_words_union = set(list(nltk_stopwords) + list(spacy_stopwords))
stop_words_final = []
for word in stop_words_union:
  if word not in spacy_stopwords:
    stop_words_final.append(word)
  if word not in nltk_stopwords:
    stop_words_final.append(word)


def cambiate2(text):
  hola=nltk.regexp_tokenize(text, pattern)
  nuevohola=[]
  for word in hola:
    if word not in stop_words_final:
      nuevohola.append(word)
  separator = ' '
  hola2=separator.join(nuevohola)
  return hola2

aysen_nuevo["SOSPECHA_DIAG"].iloc[121285]="HIPERTENSION ESENCIAL PRIMARIA cerca"

print(aysen_nuevo["SOSPECHA_DIAG"].iloc[121285])

print(cambiate2(aysen_nuevo["SOSPECHA_DIAG"].iloc[121285]))

aysen_nuevo['SOSPECHA_DIAG_NUEVO'] = aysen_nuevo.apply(lambda x: cambiate2(x.SOSPECHA_DIAG_NUEVO), axis=1)

aysen_nuevo

stemmer = nltk.stem.SnowballStemmer("spanish")
#lemmatizer = nltk.stem.WordNetLemmatizer() no hay en español

def cambiate3(text):
  hola=nltk.regexp_tokenize(text, pattern)
  nuevohola=[]
  for word in hola:
    nuevohola.append(stemmer.stem(word))
  separator = ' '
  hola2=separator.join(hola)
  return hola2

stemmer.stem("Especificada")

"""# Análisis no-supervisado y visualización (2 pts)

* Utilice en esta sección la salida de la sección de limpieza, o el texto crudo.

* Visualice utilizando wordcloud las palabras más comunes para cada
categoría D10K.
"""

list(D10K.index)

description = "Un texto de prueba para mi primer wordcloud en el todo el mundo, espero que logre repetir algunas palabras"
wc = wordcloud.WordCloud(width=1600, height=800).generate(description)
plt.figure(figsize=(10,5))
plt.imshow(wc)
plt.axis("off")
plt.show()

def plot_wordcloud_from_specialty(specialty):
  tfidf_dict = dict(zip(tfidf_vectorizer_aysen.get_feature_names(),x_aysen.toarray()[aysen.PRESTA_EST == specialty].mean(0).reshape(-1,)))
  tfidf_dict = {word:val for word,val in tfidf_dict.items() if val > 0}
  wc = wordcloud.WordCloud(width=1600, height=800).generate_from_frequencies(tfidf_dict)
  plt.figure(figsize=(10,5))
  plt.imshow(wc)
  plt.axis("off")
  plt.show()

plot_wordcloud_from_specialty('TRAUMATOLOGIA')