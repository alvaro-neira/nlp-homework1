# -*- coding: utf-8 -*-
"""Tarea_NLP_Grupo_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1keo2_83XtnNOvQdWPMoYTMwOHb2CdO-g

#Integrantes de la tarea

Integrantes del grupo:

* Integrante 1:Matias Rodriguez U. Rut: 18362815-1
* Integrante 2:Paula Canales G. Rut:18845155-1
* Integrante 3:Sebastián Alday Rut:18294095-k
* Integrante 4:Álvaro Neira R. Rut:13757209-5

# Importación de Librerias

Estas son las bibliotecas y módulos necesarios.
"""

import pandas as pd
import re

import spacy
import nltk
import sklearn
import wordcloud
import matplotlib.pyplot as plt
import random

# Datos Necesarios

# !wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/aysen.csv
#!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/aysen_corto.csv
#!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/proveedoresA.txt
#!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/other_vhosts_access.log
nltk.download('stopwords')
nltk.download('wordnet')
# !python -m spacy download es_core_news_sm
# import es_core_news_sm
# nlp_es = es_core_news_sm.load()
nlp_es = spacy.load('es_core_news_sm')

# Actividad 1: Lectura y parsing

# 1.   Utilizando la biblioteca Pandas, cargue el archivo `aysen_corto.csv` como un dataframe (revise los tipos de datos, cabecera y separador)
# 2.   Mantenga solo las columnas `sexo`, `comuna`, y `prestación`.
# 3.   Realice un conteo de los valores en de las columnas `sexo` y `comuna` y preséntelos en un gráfico.

# HINTS:
# https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html
# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html

# Prográmame

aysen = pd.read_csv("aysen.csv", sep=";", na_values=["Otro",""])

aysen.head()

##1.1 Creación del DK10

###Especialidades con más de 10.000 derivaciones


D10K_ranking=pd.DataFrame(aysen[["SOSPECHA_DIAG","PRESTA_EST"]].PRESTA_EST.value_counts(dropna=False))
D10K_ranking=D10K_ranking.loc[D10K_ranking["PRESTA_EST"]>10000]
D10K_ranking

list(D10K_ranking.index)

###Definición DK10

#Se necesita de un ciclo para recuperar la columna "SOSPECHA_DIAG" a partir de las especialidades seleccionadas por el rankin

z1=0
z2=0
presta=[]
sospecha=[]
for u in aysen["PRESTA_EST"]:
  for v in list(D10K_ranking.index):
    if u==v:
      #print(u)
      presta.append(u)
      sospecha.append(aysen["SOSPECHA_DIAG"].iloc[z1])
    z2=z2+1 
  z1=z1+1

# Se concatenan y se crea DK10

DK10=pd.concat([pd.DataFrame(sospecha).rename(columns={0:"SOSPECHA_DIAG"}),pd.DataFrame(presta).rename(columns={0:"PRESTA_EST"})],axis=1)
DK10=DK10.reset_index(drop=True)

DK10

#1.2 Transformación del texto con patrones regulares"""

# Corrígeme
# pattern = r"""(?x)                   # set flag to allow verbose regexps
#               (?:[A-Z]\.)+           # abbreviations, e.g. U.S.A.
#               |\d+(?:\.\d+)?%?       # numbers, incl. currency and percentages
#               |\w+(?:[-']\w+)*       # words w/ optional internal hyphens/apostrophe
#               |(?:[+/\-@&*])         # special characters with meanings
#             """

pattern = r"""(?x)                   # set flag to allow verbose regexps
              (?:[A-Z]\.)+           # abbreviations, e.g. U.S.A.
              |\$?\d+(?:[.,]\d+)?%?  # numbers, incl. currency and percentages
              |\w+(?:[-']\w+)*       # words w/ optional internal hyphens/apostrophe
              |(?:[+/\-@&*¡!])     # special characters with meanings
            """
pattern2 = r"""                 # set flag to allow verbose regexps
              (\s)([A-Z])         # abbreviations, e.g. U.S.A.

            """
replace= r"""$1\l$2"""
def cambiate(text):
  hola=nltk.regexp_tokenize(text, pattern)
  holanuevo=[]
  for word in hola:
      holanuevo.append(re.sub(r'([A-Z])', lambda pat: pat.group(1).lower(), word) )
  separator = ' '
  hola2=separator.join(holanuevo)
  return hola2

cambiate(DK10["SOSPECHA_DIAG"].iloc[121285])

#Se hace un experimento, el programa debería quitar la

DK10["SOSPECHA_DIAG"].iloc[121285]="HIPERTENSION ESENCIAL, PRIMARIA"

print(DK10["SOSPECHA_DIAG"].iloc[121285])

DK10['SOSPECHA_DIAG_TRANSFORMADO'] = DK10.apply(lambda x: cambiate(x.SOSPECHA_DIAG), axis=1)

DK10

"""##1.3 Transformación del texto quitando los stopwords"""

nltk_stopwords = nltk.corpus.stopwords.words('spanish')
spacy_stopwords = spacy.lang.es.stop_words.STOP_WORDS

stop_words_union = set(list(nltk_stopwords) + list(spacy_stopwords))
stop_words_final = []
for word in stop_words_union:
  if word not in spacy_stopwords:
    stop_words_final.append(word)
  if word not in nltk_stopwords:
    stop_words_final.append(word)


def cambiate2(text):
  hola=nltk.regexp_tokenize(text, pattern)
  nuevohola=[]
  for word in hola:
    if word not in stop_words_final:
      nuevohola.append(word)
  separator = ' '
  hola2=separator.join(nuevohola)
  return hola2

len(stop_words_final)

# Se hace un experimento colocando un stopword, en este caso "cerca"

DK10["SOSPECHA_DIAG"].iloc[121285]="hipertension esencial primaria cerca"

print(DK10["SOSPECHA_DIAG"].iloc[121285])

"""Después de ocupar la función, efectivamente se quita esta palabra"""

print(cambiate2(DK10["SOSPECHA_DIAG"].iloc[121285]))

"""Se hace la transformación completa."""

DK10['SOSPECHA_DIAG_TRANSFORMADO'] = DK10.apply(lambda x: cambiate2(x.SOSPECHA_DIAG_TRANSFORMADO), axis=1)

"""Se puede apreciar que se elimina la stopword, como también se puede concluir que se deberían quitar las demás en el caso de que las haya."""

DK10

"""##1.4 Transformación del texto con stemmer"""

stemmer = nltk.stem.SnowballStemmer("spanish")
#lemmatizer = nltk.stem.WordNetLemmatizer() no hay en español

def cambiate3(text):
  hola=nltk.regexp_tokenize(text, pattern)
  nuevohola=[]
  for word in hola:
    nuevohola.append(stemmer.stem(word))
  separator = ' '
  hola2=separator.join(nuevohola)
  return hola2

stemmer.stem("especificada")

DK10['SOSPECHA_DIAG_TRANSFORMADO'] = DK10.apply(lambda x: cambiate3(x.SOSPECHA_DIAG_TRANSFORMADO), axis=1)

DK10

"""##DK10 Crudo y Procesado"""

DK10_crudo=DK10[["SOSPECHA_DIAG","PRESTA_EST"]].copy()
DK10_crudo

DK10_procesado=DK10[["SOSPECHA_DIAG_TRANSFORMADO","PRESTA_EST"]].copy()
DK10_procesado=DK10_procesado.rename(columns={"SOSPECHA_DIAG_TRANSFORMADO":"SOSPECHA_DIAG"})
DK10_procesado

"""#  2 Análisis no-supervisado y visualización

* Utilice en esta sección la salida de la sección de limpieza, o el texto crudo.

### 2.1 Visualice utilizando wordcloud las palabras más comunes para cada categoría D10K.

* En esta parte me baso en **2-normalization_and_vector_semantics.sol.ipynb**

* En esta seccion utilizamos los stopwords calculados en el punto 1.3
"""

stopwords_aysen = stop_words_union

def plot_wordcloud_from_specialty(dataset, specialty):
  tfidf_dict = dict(zip(tfidf_vectorizer_aysen.get_feature_names_out(),x_aysen.toarray()[dataset.PRESTA_EST == specialty].mean(0).reshape(-1,)))
  tfidf_dict = {word:val for word,val in tfidf_dict.items() if val > 0}
  wc = wordcloud.WordCloud(width=1600, height=800).generate_from_frequencies(tfidf_dict)
  plt.figure(figsize=(10,5))
  plt.imshow(wc)
  plt.axis("off")
  plt.show()

tfidf_vectorizer_aysen = sklearn.feature_extraction.text.TfidfVectorizer(stop_words=stopwords_aysen,max_df=0.20)
x_aysen = tfidf_vectorizer_aysen.fit_transform(DK10_procesado.SOSPECHA_DIAG)

tfidf_vectorizer_raw = sklearn.feature_extraction.text.TfidfVectorizer(stop_words=stopwords_aysen,max_df=0.20)
x_raw = tfidf_vectorizer_raw.fit_transform(DK10_crudo.SOSPECHA_DIAG)
def plot_wordcloud_from_specialty_raw(dataset, specialty):
  tfidf_dict = dict(zip(tfidf_vectorizer_raw.get_feature_names_out(),x_raw.toarray()[dataset.PRESTA_EST == specialty].mean(0).reshape(-1,)))
  tfidf_dict = {word:val for word,val in tfidf_dict.items() if val > 0}
  wc = wordcloud.WordCloud(width=1600, height=800).generate_from_frequencies(tfidf_dict)
  plt.figure(figsize=(10,5))
  plt.imshow(wc)
  plt.axis("off")
  plt.show()

for specialty in list(D10K_ranking.index):
  print(f"DK10_procesado: {specialty}")
  plot_wordcloud_from_specialty(DK10_procesado, specialty)
  print(f"")
  print(f"DK10_crudo: {specialty}")
  plot_wordcloud_from_specialty(DK10_crudo, specialty)
  print(f"")
