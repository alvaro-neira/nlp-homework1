{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_part_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I8dba1bg9zI"
      },
      "source": [
        "Importación de librerías."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21BbHj_N7G0D"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import gc\n",
        "import spacy\n",
        "import nltk\n",
        "import sklearn\n",
        "import wordcloud\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lexwob8PrGvc"
      },
      "source": [
        "Descarga de los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF7EbQTkqeBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc04f6e-1d0b-4238-e3b9-51f2b44a20ba"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/aysen.csv\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "!python -m spacy download es_core_news_sm\n",
        "import es_core_news_sm\n",
        "nlp_es = es_core_news_sm.load()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-04 23:38:16--  https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/aysen.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29482924 (28M) [text/plain]\n",
            "Saving to: ‘aysen.csv’\n",
            "\n",
            "aysen.csv           100%[===================>]  28.12M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-12-04 23:38:18 (216 MB/s) - ‘aysen.csv’ saved [29482924/29482924]\n",
            "\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Collecting es_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.2 MB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2021.10.8)\n",
            "Building wheels for collected packages: es-core-news-sm\n",
            "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.2.5-py3-none-any.whl size=16172933 sha256=c02199ffc10555373a4b7e8a498afc1f9826448920ad92c99398773578436902\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ahqhym7a/wheels/21/8d/a9/6c1a2809c55dd22cd9644ae503a52ba6206b04aa57ba83a3d8\n",
            "Successfully built es-core-news-sm\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8a7I8BEfl87"
      },
      "source": [
        "# 1. Limpieza de la base de datos\n",
        "\n",
        "Este capítulo del trabajo consiste en la limpieza de la base de datos de la lista de espera de la Región de Aysén. Se parte utilizando sólo las columnas de sospecha diagnóstica y la prestación a la que se deriva, para luego seleccionar las que tienen una cantidad mayor a 10.000 derivaciones y realizar un preprocesamiento del texto. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDDUUTUtrN3E"
      },
      "source": [
        "## Carga base de datos Aysén\n",
        "\n",
        "En primer lugar, se inspecciona la base de datos cargada y luego se seleccionan sólo las columnas \"SOSPECHA_DIAG\" y \"PRESTA_EST\". Estas columnas son ingresadas a la variable 'aysen_subset'. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lyv2yDdu-GX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c516f1e7-a0c9-4350-9cce-b7962fb8ab64"
      },
      "source": [
        "aysen = pd.read_csv(\"aysen.csv\", sep=\";\", na_values=[\"Otro\",\"\"])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0khQvBnznFPV"
      },
      "source": [
        "Selección de las columnas \"SOSPECHA_DIAG\" (sospecha diagnóstica) y \"PRESTA_EST\" (prestación)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2VcHGbyvOFK"
      },
      "source": [
        "aysen_subset = aysen[[\"SOSPECHA_DIAG\",\"PRESTA_EST\"]]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYaIEhv4ob4-"
      },
      "source": [
        "## 1.1 Derivaciones mayores a 10.000 - D10K \n",
        "\n",
        "En el presente apartado se cuentas las derivaciones a cada especialidad y se realiza un ránking con ellas.\n",
        "\n",
        "Se calcula el dataframe D10K que contiene las prestaciones con más de 10.000 valores. Las especialidades se muestran de forma individual. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "vFjnHRr53n_0",
        "outputId": "a574a7ef-eac8-4fc0-8e19-4aaa5b0417de"
      },
      "source": [
        "D10K_rank = pd.DataFrame(aysen_subset.PRESTA_EST.value_counts(dropna=False))\n",
        "D10K_rank = D10K_rank.loc[D10K_rank[\"PRESTA_EST\"] > 10000]\n",
        "D10K_rank = D10K_rank.reset_index()\n",
        "D10K_rank.columns = [\"PRESTA_EST\", \"PRESTA_EST_COUNT\"]\n",
        "D10K_rank\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PRESTA_EST</th>\n",
              "      <th>PRESTA_EST_COUNT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAUMATOLOGIA</td>\n",
              "      <td>24004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OFTALMOLOGIA</td>\n",
              "      <td>21482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CIRUGIA ADULTO</td>\n",
              "      <td>17321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OTORRINOLARINGOLOGIA</td>\n",
              "      <td>13663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NEUROLOGIA</td>\n",
              "      <td>12316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>MEDICINA INTERNA</td>\n",
              "      <td>11408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GINECOLOGIA</td>\n",
              "      <td>10871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ENDODONCIA</td>\n",
              "      <td>10225</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             PRESTA_EST  PRESTA_EST_COUNT\n",
              "0         TRAUMATOLOGIA             24004\n",
              "1          OFTALMOLOGIA             21482\n",
              "2        CIRUGIA ADULTO             17321\n",
              "3  OTORRINOLARINGOLOGIA             13663\n",
              "4            NEUROLOGIA             12316\n",
              "5      MEDICINA INTERNA             11408\n",
              "6           GINECOLOGIA             10871\n",
              "7            ENDODONCIA             10225"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR4VS75Q68Dk",
        "outputId": "c4950198-73a4-41fd-8e45-327c59dc5849"
      },
      "source": [
        "list(D10K_rank.PRESTA_EST)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TRAUMATOLOGIA',\n",
              " 'OFTALMOLOGIA',\n",
              " 'CIRUGIA ADULTO',\n",
              " 'OTORRINOLARINGOLOGIA',\n",
              " 'NEUROLOGIA',\n",
              " 'MEDICINA INTERNA',\n",
              " 'GINECOLOGIA',\n",
              " 'ENDODONCIA']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtgOym_3c932"
      },
      "source": [
        "D10K = pd.merge(aysen_subset, D10K_rank[\"PRESTA_EST\"], on=\"PRESTA_EST\", how='right')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwPy1fuSrYmK"
      },
      "source": [
        "## 1.2 Normalización del texto\n",
        "\n",
        "A continuación, se construyen las expresiones regulares necesarias para:\n",
        "- Eliminar signos de puntuación.\n",
        "- Cambiar todo el texto a minúsculas.\n",
        "- Cambiar cualquier símbolo no-ASCII."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG0kB9guBF4E"
      },
      "source": [
        "pattern = r\"\"\"(?x)                   # set flag to allow verbose regexps\n",
        "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A.\n",
        "              |\\$?\\d+(?:[.,]\\d+)?%?  # numbers, incl. currency and percentages\n",
        "              |\\w+(?:[-']\\w+)*       # words w/ optional internal hyphens/apostrophe\n",
        "              |(?:[+/\\-@&*¡!.,])     # special characters with meanings\n",
        "            \"\"\"\n",
        "\n",
        "def normalizer(text, remove_tildes = True): #normalizes a given string to lowercase and changes all vowels to their base form\n",
        "    text = text.lower() #string lowering\n",
        "    text = re.sub(r'[^A-Za-zñáéíóú]', ' ', text) #replaces every punctuation with a space\n",
        "    text = nltk.regexp_tokenize(text, pattern)\n",
        "    text = ' '.join(text)\n",
        "    if remove_tildes:\n",
        "        text = re.sub('á', 'a', text) #replaces special vowels to their base forms\n",
        "        text = re.sub('é', 'e', text)\n",
        "        text = re.sub('í', 'i', text)\n",
        "        text = re.sub('ó', 'o', text)\n",
        "        text = re.sub('ú', 'u', text)\n",
        "    return text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO3tzP8S-vmB"
      },
      "source": [
        "D10K['SOSPECHA_DIAG_NORM'] = D10K.apply(lambda x: normalizer(x.SOSPECHA_DIAG), axis=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqxR4eHzB5QE"
      },
      "source": [
        "## 1.3. Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9JQ1t55E2Nl"
      },
      "source": [
        "<font color='red'>**No entendí muy bien por qué se hizo esta unión de palabras que no están en común en la lista. Yo lo cambiaré y simplemente utilizaré simplemente la unión de las dos listas. +1 de Alvaro.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTiCwtEO5evH"
      },
      "source": [
        "En primer lugar, se escoge la lista de stopwords a utilizar. En este caso se eligen dos listas de stopwords: nltk y spacy. Se importan las dos listas de stopwords y luego se realiza una unión de los dos conjutos de palabras. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkKYisJbB6F4"
      },
      "source": [
        "nltk_stopwords = nltk.corpus.stopwords.words('spanish')\n",
        "spacy_stopwords = spacy.lang.es.stop_words.STOP_WORDS\n",
        "\n",
        "stop_words = set(list(nltk_stopwords) + list(spacy_stopwords))\n",
        "stop_words = list(stop_words)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLSHjN-b6W2y"
      },
      "source": [
        "El conjunto de stopwords debe ser normalizado de la misma forma en que se normalizó el texto de la sospecha diagnóstica. Luego de esta normalización se eliminan los stopwords de la columna \"SOSPECHA_DIAG_NORM\" en D10K. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GNwutkH5aG-"
      },
      "source": [
        "stopwords_normalized = [normalizer(word) for word in stop_words]\n",
        "\n",
        "def no_stopwords(text):\n",
        "  '''\n",
        "  Función para eliminar las stopwords de un texto\n",
        "  '''\n",
        "  text = text.split(\" \")\n",
        "  text_nosw = []\n",
        "  for w in range(len(text)):\n",
        "    if text[w] not in stopwords_normalized:\n",
        "      text_nosw.append(text[w])\n",
        "  separator = ' '\n",
        "  text = separator.join(text_nosw)\n",
        "  return text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibZF9r9BAW5m"
      },
      "source": [
        "La función descrita anteriormente permite eliminar los stopwords de un texto normalizado. Se presenta un ejemplo de esto:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4AyMWa7BW0k"
      },
      "source": [
        "Se aplica la función para quitar las stopwords a todas las filas de \"SOSPECHA_DIAG_NORM\" y se guardan en una nueva columna llamada \"SOSPECHA_DIAG_NOSTOPWORDS\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8WPNq0Mon7e"
      },
      "source": [
        "# 3. Análisis supervisado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE4a9nBMouX6"
      },
      "source": [
        "## 3.1 Probabilidad de cada categoría en D10K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IymlcOxpIqW"
      },
      "source": [
        "Para calcular la probabilidad de cada categoría en D10K, primero obtendremos el total de categorías para luego dividir la cuenta de categoría por el total. Asignaremos una nueva columna en el dataframe para la probabilidad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "wSIc8o9xoxA_",
        "outputId": "2b275ca7-80c5-4d91-e1c6-0ba3c36349cf"
      },
      "source": [
        "total_cat = D10K_rank.PRESTA_EST_COUNT.sum()\n",
        "D10K_rank['PROB'] = D10K_rank.PRESTA_EST_COUNT/total_cat\n",
        "D10K_rank"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PRESTA_EST</th>\n",
              "      <th>PRESTA_EST_COUNT</th>\n",
              "      <th>PROB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAUMATOLOGIA</td>\n",
              "      <td>24004</td>\n",
              "      <td>0.197906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OFTALMOLOGIA</td>\n",
              "      <td>21482</td>\n",
              "      <td>0.177113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CIRUGIA ADULTO</td>\n",
              "      <td>17321</td>\n",
              "      <td>0.142806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OTORRINOLARINGOLOGIA</td>\n",
              "      <td>13663</td>\n",
              "      <td>0.112647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NEUROLOGIA</td>\n",
              "      <td>12316</td>\n",
              "      <td>0.101542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>MEDICINA INTERNA</td>\n",
              "      <td>11408</td>\n",
              "      <td>0.094056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GINECOLOGIA</td>\n",
              "      <td>10871</td>\n",
              "      <td>0.089628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ENDODONCIA</td>\n",
              "      <td>10225</td>\n",
              "      <td>0.084302</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             PRESTA_EST  PRESTA_EST_COUNT      PROB\n",
              "0         TRAUMATOLOGIA             24004  0.197906\n",
              "1          OFTALMOLOGIA             21482  0.177113\n",
              "2        CIRUGIA ADULTO             17321  0.142806\n",
              "3  OTORRINOLARINGOLOGIA             13663  0.112647\n",
              "4            NEUROLOGIA             12316  0.101542\n",
              "5      MEDICINA INTERNA             11408  0.094056\n",
              "6           GINECOLOGIA             10871  0.089628\n",
              "7            ENDODONCIA             10225  0.084302"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZhJKN-5oxTG"
      },
      "source": [
        "## 3.2 Diccionario P(palabra|categoria)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldfd0BGaqCrs"
      },
      "source": [
        "En esta sección se construirá un diccionario, donde para cada palabra se obtendrá la probabilidad P(palabra|categoria).\n",
        "[IMAGEN]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U5_A0YTqlwS"
      },
      "source": [
        "Primero dividiremos la data de D10K en entrenamiento y test (90% y 10% respectivamente)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwfJPr8ko0ns"
      },
      "source": [
        "train, test = train_test_split(D10K, test_size=0.1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XCgkFe_tAe-"
      },
      "source": [
        "A continuación, se construye el vector en base a los datos de entrenamiento quitando los stop_words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDdqsTTStKcq",
        "outputId": "2d129827-5c2a-4055-bd3c-61cfbb6efdc3"
      },
      "source": [
        "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(stop_words=stop_words)\n",
        "term_doc_matrix_train = count_vectorizer.fit_transform(train.SOSPECHA_DIAG)\n",
        "term_doc_matrix_train"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<109161x4274 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 303353 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOOZ3598quUS"
      },
      "source": [
        "Luego, se calcula la probabilidad de cada palabra por categoría"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtfkS83uqsvt"
      },
      "source": [
        "p_w_category = {}\n",
        "p_w_s_category = {}\n",
        "for index, row in D10K_rank.iterrows():\n",
        "  category = row.PRESTA_EST\n",
        "  count_c = np.asarray(term_doc_matrix_train.todense()[train.PRESTA_EST == category,:].sum(0)).reshape(-1).sum()\n",
        "  # Se obtienen las apariciones de cada una de las palabras en cada clase\n",
        "  c_w = np.asarray(term_doc_matrix_train.todense()[train.PRESTA_EST == category,:].sum(0)).reshape(-1)\n",
        "  # Se calcula la probabilidad normal\n",
        "  p_w = c_w / count_c\n",
        "  # Se calcula la probabilidad suavizada para bayes\n",
        "  vocab = len(np.asarray(term_doc_matrix_train.todense()[train.PRESTA_EST == category,:].sum(0)).reshape(-1))\n",
        "  p_w_smoothing = (c_w + 1) / (count_c + vocab)\n",
        "  \n",
        "  p_w_category[category] = dict(zip(count_vectorizer.get_feature_names_out(), p_w))\n",
        "  p_w_s_category[category] = dict(zip(count_vectorizer.get_feature_names_out(), p_w_smoothing))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDjt5g1n_vnd"
      },
      "source": [
        "## 3.3 *Construya un clasificador Bayesiano Naive. Reporte el resultado de 3 métricas de clasificación ¿Detecto alguna categoría de mayor dificultad? ¿puede sugerir alguna interpretación a su resultado?*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZnPVP3n0R3q"
      },
      "source": [
        "c_w_negative = np.asarray(term_doc_matrix_train.todense()[train.PRESTA_EST != 'TRAUMATOLOGIA', :].sum(0)).reshape(-1)\n",
        "c_w_positive = np.asarray(term_doc_matrix_train.todense()[train.PRESTA_EST == 'TRAUMATOLOGIA', :].sum(0)).reshape(-1)\n",
        "c_negative = (train.PRESTA_EST != 'TRAUMATOLOGIA').sum()\n",
        "c_positive = (train.PRESTA_EST == 'TRAUMATOLOGIA').sum()\n",
        "p_w_negative = (c_w_negative + 1) / c_negative # Probabilidad asociada a cada palabra en la clase negativa\n",
        "p_w_positive = (c_w_positive + 1) / c_positive\n",
        "r = np.log(p_w_positive / p_w_negative)\n",
        "term_doc_matrix_test = count_vectorizer.transform(test.SOSPECHA_DIAG)\n",
        "test_pred = term_doc_matrix_test.dot(r) > 0\n",
        "test_label = np.where(test.PRESTA_EST == 'TRAUMATOLOGIA', True, False)\n",
        "confusion_matrix = pd.crosstab(test_label,test_pred)\n",
        "TP = confusion_matrix[1][1]\n",
        "FP = confusion_matrix[1][0]\n",
        "FN = confusion_matrix[0][1]\n",
        "accuracy = (np.array(confusion_matrix).diagonal().sum() / np.array(confusion_matrix).sum())\n",
        "precision = TP/(TP+FP)\n",
        "recall = TP/(TP+FN)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKBQp81g03Q-",
        "outputId": "1ced50c6-e663-47f8-d216-5fb8485de1c2"
      },
      "source": [
        "print(f\"accuracy={accuracy}, precision={precision}, recall={recall}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy=0.854151207848957, precision=0.6105436573311367, recall=0.7594262295081967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JUJ4UTGLo1h"
      },
      "source": [
        "for name, values in train.iteritems():\n",
        "  if name not in ['PRESTA_EST', 'SOSPECHA_DIAG']:\n",
        "    train.drop(name, axis=1, inplace=True)\n",
        "\n",
        "for name, values in test.iteritems():\n",
        "  if name not in ['PRESTA_EST', 'SOSPECHA_DIAG']:\n",
        "    test.drop(name, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWzvF-R_7i2X"
      },
      "source": [
        "def binary_naive_bayes(count_vectorizer, train_matrix, specialty):\n",
        "  c_w_negative = np.asarray(train_matrix.todense()[train.PRESTA_EST != specialty, :].sum(0)).reshape(-1)\n",
        "  c_w_positive = np.asarray(train_matrix.todense()[train.PRESTA_EST == specialty, :].sum(0)).reshape(-1)\n",
        "  c_negative = (train.PRESTA_EST != specialty).sum()\n",
        "  c_positive = (train.PRESTA_EST == specialty).sum()\n",
        "  p_w_negative = (c_w_negative + 1) / c_negative # Probabilidad asociada a cada palabra en la clase negativa\n",
        "  p_w_positive = (c_w_positive + 1) / c_positive\n",
        "  r = np.log(p_w_positive / p_w_negative)\n",
        "  term_doc_matrix_test = count_vectorizer.transform(test.SOSPECHA_DIAG)\n",
        "  test_pred = term_doc_matrix_test.dot(r) > 0\n",
        "  test_label = np.where(test.PRESTA_EST == specialty, True, False)\n",
        "  confusion_matrix = pd.crosstab(test_label,test_pred)\n",
        "  TP = confusion_matrix[1][1]\n",
        "  FP = confusion_matrix[1][0]\n",
        "  FN = confusion_matrix[0][1]\n",
        "  accuracy = (np.array(confusion_matrix).diagonal().sum() / np.array(confusion_matrix).sum())\n",
        "  precision = TP/(TP+FP)\n",
        "  recall = TP/(TP+FN)\n",
        "  return accuracy, precision, recall"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puNB1KDC7rWr",
        "outputId": "3264c431-0b0a-47a2-eefd-7ba8321e3e93"
      },
      "source": [
        "for specialty in list(D10K_rank.PRESTA_EST):\n",
        "  accuracy, precision, recall = binary_naive_bayes(count_vectorizer, term_doc_matrix_train, specialty)\n",
        "  print(f\"specialty: {specialty}: accuracy={accuracy}, precision={precision}, recall={recall}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "specialty: TRAUMATOLOGIA: accuracy=0.854151207848957, precision=0.6105436573311367, recall=0.7594262295081967\n",
            "specialty: OFTALMOLOGIA: accuracy=0.9154917965207354, precision=0.7551963048498845, recall=0.7676056338028169\n",
            "specialty: CIRUGIA ADULTO: accuracy=0.7186907411987797, precision=0.3257992262268377, recall=0.940623162845385\n",
            "specialty: OTORRINOLARINGOLOGIA: accuracy=0.9147497732706736, precision=0.6055261610817166, recall=0.7394113424264178\n",
            "specialty: NEUROLOGIA: accuracy=0.8341165800972875, precision=0.3522012578616352, recall=0.7848309975267931\n",
            "specialty: MEDICINA INTERNA: accuracy=0.6840629895292275, precision=0.2216842105263158, recall=0.8863636363636364\n",
            "specialty: GINECOLOGIA: accuracy=0.694698656113447, precision=0.224468309117709, recall=0.9815837937384899\n",
            "specialty: ENDODONCIA: accuracy=0.9598482974688762, precision=0.7362848893166506, recall=0.7822085889570553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IC-yUSRaTvo",
        "outputId": "fd400b8b-455a-47da-d646-65cc10221526"
      },
      "source": [
        "count_vectorizer = None\n",
        "term_doc_matrix_train = None\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "190"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpxm01nu8dee"
      },
      "source": [
        "## 3.4 *¿Cómo varía su resultado al utilizar bigramas?*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBRvYhJWVv_9"
      },
      "source": [
        "count_vectorizer_bigrams = sklearn.feature_extraction.text.CountVectorizer(stop_words=stop_words, ngram_range=(1, 2))\n",
        "term_doc_matrix_train_bigrams = count_vectorizer_bigrams.fit_transform(train.SOSPECHA_DIAG)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt1-4srXanLx",
        "outputId": "1faa8563-ea81-4777-c341-95f737db5791"
      },
      "source": [
        "for specialty in list(D10K_rank.PRESTA_EST):\n",
        "  accuracy, precision, recall=binary_naive_bayes(count_vectorizer_bigrams, term_doc_matrix_train_bigrams, specialty)\n",
        "  print(f\"specialty (with bigrams): {specialty}: accuracy={accuracy}, precision={precision}, recall={recall}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "specialty (with bigrams): TRAUMATOLOGIA: accuracy=0.8507708797097865, precision=0.6007677543186181, recall=0.769672131147541\n",
            "specialty (with bigrams): OFTALMOLOGIA: accuracy=0.9157391376040894, precision=0.7555350553505535, recall=0.7690140845070422\n",
            "specialty (with bigrams): CIRUGIA ADULTO: accuracy=0.7203396817544727, precision=0.3275545584336121, recall=0.9441504997060552\n",
            "specialty (with bigrams): OTORRINOLARINGOLOGIA: accuracy=0.9042790007420233, precision=0.5618336886993603, recall=0.756640344580043\n",
            "specialty (with bigrams): NEUROLOGIA: accuracy=0.8269436886800231, precision=0.3416726233023588, recall=0.7881286067600989\n",
            "specialty (with bigrams): MEDICINA INTERNA: accuracy=0.6732624288894385, precision=0.21770091556459817, recall=0.9006734006734006\n",
            "specialty (with bigrams): GINECOLOGIA: accuracy=0.6892571522796603, precision=0.2217382285832815, recall=0.9843462246777164\n",
            "specialty (with bigrams): ENDODONCIA: accuracy=0.9538296644405969, precision=0.6876122082585279, recall=0.7832310838445807\n"
          ]
        }
      ]
    }
  ]
}