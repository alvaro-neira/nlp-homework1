# -*- coding: utf-8 -*-
"""Entrega_no_red_cross.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hSnmfwUVRu8SudCvqU4WovHPsPKXx5ly

# No Red Cross ~10 minutos

Integrantes:

* Sebastián Alday (Rut: 18294095-K)
* Paula Canales G. (Rut: 18845155-1)
* Álvaro Neira R. (Rut: 13757209-5)
* Matias Rodriguez U. (Rut: 18362815-1)

# Introducción

En el siguiente documento se describe la tarea 1 para el curso Procesamiento de Lenguaje Natural del Diplomado Inteligencia Artificial Primavera 2021.

El objetivo es visualizar y predecir las derivaciones de los médicos de los servicios primarios de salud (CESFAM) en base a la sospecha diagnóstica. Se dispone de una base de datos de la lista de espera entre 2012 y 2017 para la región de Aysen (datos obtenidos vía ley de transparencia).

Para el desarrollo de este trabajo se utilizaron técnicas de limpieza de datos, análisis no supervisado, visualización y análisis supervisado.

# Carga de módulos, bibliotecas e información necesaria

Importación de librerías.
"""

import pandas as pd
import re
import spacy
import nltk
import sklearn
import wordcloud
import matplotlib.pyplot as plt
import pandas as pd
import random
import numpy as np
import json
import gc
from sklearn.model_selection import train_test_split

"""Descarga de los datos."""

!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/aysen.csv
!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/aysen_corto.csv
!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/proveedoresA.txt
!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/data/other_vhosts_access.log
nltk.download('stopwords')
nltk.download('wordnet')
!python -m spacy download es_core_news_sm
import es_core_news_sm
nlp_es = es_core_news_sm.load()

"""# 1. Limpieza de la base de datos

Este capítulo del trabajo consiste en la limpieza de la base de datos de la lista de espera de la Región de Aysén. Se parte utilizando sólo las columnas de sospecha diagnóstica y la prestación a la que se deriva, para luego seleccionar las que tienen una cantidad mayor a 10.000 derivaciones y realizar un preprocesamiento del texto.

## Carga base de datos Aysén

En primer lugar, se inspecciona la base de datos cargada y luego se seleccionan sólo las columnas "SOSPECHA_DIAG" y "PRESTA_EST". Estas columnas son ingresadas a la variable 'aysen_subset'.
"""

aysen = pd.read_csv("aysen.csv", sep=";", na_values=["Otro",""])

"""Inspección del dataframe aysen.csv"""

aysen.head()

"""Selección de las columnas "SOSPECHA_DIAG" (sospecha diagnóstica) y "PRESTA_EST" (prestación)."""

aysen_subset = aysen[["SOSPECHA_DIAG","PRESTA_EST"]]

aysen_subset.head()

"""## 1.1 Derivaciones mayores a 10.000 - D10K 

En el presente apartado se cuentas las derivaciones a cada especialidad y se realiza un ránking con ellas.

Se calcula el dataframe D10K que contiene las prestaciones con más de 10.000 valores. Las especialidades se muestran de forma individual. 
"""

D10K_rank = pd.DataFrame(aysen_subset.PRESTA_EST.value_counts(dropna=False))
D10K_rank = D10K_rank.loc[D10K_rank["PRESTA_EST"] > 10000]
D10K_rank = D10K_rank.reset_index()
D10K_rank.columns = ["PRESTA_EST", "PRESTA_EST_COUNT"]
D10K_rank

list(D10K_rank.PRESTA_EST)

"""Se realiza una selección de las sospechas diagnósticas correspondientes a las 8 prestaciones con más que 10.000 atenciones. """

D10K = pd.merge(aysen_subset, D10K_rank["PRESTA_EST"], on="PRESTA_EST", how='right')

D10K

"""## 1.2 Normalización del texto

A continuación, se construyen las expresiones regulares necesarias para:
- Eliminar signos de puntuación.
- Cambiar todo el texto a minúsculas.
- Cambiar cualquier símbolo no-ASCII.

Para aplicar estas expresiones regulares se crea la función normalizer.
"""

pattern = r"""(?x)                   # set flag to allow verbose regexps
              (?:[A-Z]\.)+           # abbreviations, e.g. U.S.A.
              |\$?\d+(?:[.,]\d+)?%?  # numbers, incl. currency and percentages
              |\w+(?:[-']\w+)*       # words w/ optional internal hyphens/apostrophe
              |(?:[+/\-@&*¡!.,])     # special characters with meanings
            """

def normalizer(text, remove_tildes = True): #normalizes a given string to lowercase and changes all vowels to their base form
    text = text.lower() #string lowering
    text = re.sub(r'[^A-Za-zñáéíóú]', ' ', text) #replaces every punctuation with a space
    text = nltk.regexp_tokenize(text, pattern)
    text = ' '.join(text)
    if remove_tildes:
        text = re.sub('á', 'a', text) #replaces special vowels to their base forms
        text = re.sub('é', 'e', text)
        text = re.sub('í', 'i', text)
        text = re.sub('ó', 'o', text)
        text = re.sub('ú', 'u', text)
    return text

"""Ejemplo de normalización de una sospecha diagnóstica."""

D10K["SOSPECHA_DIAG"].iloc[5]

normalizer(D10K["SOSPECHA_DIAG"].iloc[5], remove_tildes=True)

"""La función normalizer es aplicada a la columna SOSPECHA_DIAG y se guarda en la columna SOSPECHA_DIAG_NORM."""

D10K['SOSPECHA_DIAG_NORM'] = D10K.apply(lambda x: normalizer(x.SOSPECHA_DIAG), axis=1)

D10K

"""## 1.3. Stopwords

En primer lugar, se escoge la lista de stopwords a utilizar. En este caso se eligen dos listas de stopwords: nltk y spacy. Se importan las dos listas de stopwords y luego se realiza una unión de los dos conjutos de palabras.
"""

nltk_stopwords = nltk.corpus.stopwords.words('spanish')
spacy_stopwords = spacy.lang.es.stop_words.STOP_WORDS

stop_words = set(list(nltk_stopwords) + list(spacy_stopwords))
stop_words = list(stop_words)

print(random.sample(stop_words,10))

"""El conjunto de stopwords debe ser normalizado de la misma forma en que se normalizó el texto de la sospecha diagnóstica. Luego de esta normalización se eliminan los stopwords de la columna "SOSPECHA_DIAG_NORM" en D10K. """

stopwords_normalized = [normalizer(word) for word in stop_words]

def no_stopwords(text):
  '''
  Función para eliminar las stopwords de un texto
  '''
  text = text.split(" ")
  text_nosw = []
  for w in range(len(text)):
    if text[w] not in stopwords_normalized:
      text_nosw.append(text[w])
  separator = ' '
  text = separator.join(text_nosw)
  return text

"""La función descrita anteriormente permite eliminar los stopwords de un texto normalizado. Se presenta un ejemplo de esto:"""

print(D10K["SOSPECHA_DIAG_NORM"].iloc[1])

print(no_stopwords(D10K["SOSPECHA_DIAG_NORM"].iloc[1]))

"""Se aplica la función para quitar las stopwords a todas las filas de "SOSPECHA_DIAG_NORM" y se guardan en una nueva columna llamada "SOSPECHA_DIAG_NOSTOPWORDS"."""

D10K['SOSPECHA_DIAG_NOSTOPWORDS'] = D10K.apply(lambda x: no_stopwords(x.SOSPECHA_DIAG_NORM), axis=1)

D10K

"""## 1.4 Stemming del texto

En esta sección se ha escogido realizar stemming del texto mediante la librería nltk SnowballStemmer en español. Para ello se crea primero la función stemming.
"""

stemmer = nltk.stem.SnowballStemmer("spanish")

def stemming(text):
  text = text.split(" ")
  stemmed = []
  for word in text:
    stemmed.append(stemmer.stem(word))
  separator = ' '
  text = separator.join(stemmed)
  return text

"""Un ejemplo de cómo se comporta esta función es el siguiente:"""

print(stemming(D10K["SOSPECHA_DIAG_NOSTOPWORDS"].iloc[121285]))

"""Se aplica la función stemming a todas las sospechas diagnósticas que ya han sido normalizadas y quitado las stopwords. """

D10K['SOSPECHA_DIAG_STEM'] = D10K.apply(lambda x: stemming(x.SOSPECHA_DIAG_NOSTOPWORDS), axis=1)

"""# 2. Análisis no-supervisado y visualización

Esta sección corresponde al análisis no-supervisado y de visualización mediante técnicas como WordCloud, TF-IDF y matriz término-documento. El objetivo de este análisis es realizar una primera aproximación a las características de la base de datos de la lista de espera de la Región de Aysén.

## 2.1 WordCloud de palabras más comunes para cada especialidad en D10K

Se crea la función plot_wordcloud_from_specialty_count, considerando en el tamaño de las palabras la cantidad de apariciones y la columna de D10K sin stop words. Se obtiene una imagen para cada una de las especialidades de derivación.
"""

def plot_wordcloud_from_specialty_count(specialty):
  D10K_specialty = D10K[D10K['PRESTA_EST'] == specialty]
  corpus = " ".join(D10K_specialty.SOSPECHA_DIAG_NOSTOPWORDS.to_list())
  wc = wordcloud.WordCloud(width=1600, height=800).generate(corpus)
  plt.figure(figsize=(10,5))
  plt.imshow(wc)
  plt.axis("off")
  plt.show()

"""En las siguientes representaciones WordCloud se observa que las palabras mayoritarias por conteo son muy similares en todas las categorías. Las palabras que representan a cada especialidad se presentan en tamaños pequeños. """

for specialty in list(D10K_rank.PRESTA_EST):
  print(f"{specialty}")
  plot_wordcloud_from_specialty_count(specialty)
  print(f"")

"""## 2.2 Matrices término-documento y TF-IDF en D10K

En la presente sección se calculan las matrices término-documento y TF-IDF para el dataframe D10K que ha sido normalizado en la sección 1 de este trabajo. Esto quiere decir que se utiliza la columna "SOSPECHA_DIAG_NORM". 

Se realiza la vectorización TF-IDF mediante el método implementado en sklearn.
"""

tfidf_vectorizer_D10K = sklearn.feature_extraction.text.TfidfVectorizer(stop_words=stopwords_normalized,max_df=0.20)
x_D10K = tfidf_vectorizer_D10K.fit_transform(D10K.SOSPECHA_DIAG_NORM)

"""Se imprimen las palabras más comunes y las menos comunes dentro de la columno de Sospecha diagnóstico."""

idf_dict = dict(zip(tfidf_vectorizer_D10K.get_feature_names(),tfidf_vectorizer_D10K.idf_))
idf_dict = {k: v for k, v in sorted(idf_dict.items(), key=lambda item: item[1])}
print(list(idf_dict.items())[:5])
print(list(idf_dict.items())[-5:])

"""Se imprimen las matrices TF-IDF y término-documento.

En este caso, cada documento es una sospecha de diagnóstico. 

Finalmente, se muestra el tamaño de la matriz y el largo del vocabulario.
"""

print(x_D10K) # por defecto el objeto x es una estructura iddoc, idpalabra, tfidf
print(x_D10K.toarray()) # para visualizarlo como matriz hay que utilizar el método scipy.sparse.csr_matrix.toarray()

print(x_D10K.shape) # dimensiones de la matriz
print(len(tfidf_vectorizer_D10K.get_feature_names())) # el tamaño de vocabulario coincide con el número de filas
print(random.sample(tfidf_vectorizer_D10K.get_feature_names(),10))

"""## 2.3 SVD sobre la matriz TF-IDF

En la presente sección se realiza una descomposición SVD sobre la matriz TF-IDF. Con el resultado obtenido se verifica si los temas principales identificados corresponden con las prestaciones médicas a las cuales son derivados dependiendo de la sospecha diagnóstica. 

Se hace la descomposición SVD sobre TF-IDF:
"""

#SVD ~5 minutos
U, s, Vh = np.linalg.svd(x_D10K.toarray(), full_matrices=False)

"""Además, se recupera el vocabulario correspondiente a la columna "SOSPECHA_DIAG_NORM" del dataframe D10K. Posteriormente, se define la función show_topics, que permite obtner las palabras más representativas de los temas identificados en la descomposición SVD. """

vocab_D10K = tfidf_vectorizer_D10K.get_feature_names()
def show_topics(a):
  num_top_words=8
  top_words_values = lambda t: [f"{vocab_D10K[i]}({str(round(v,3))})" for i,v in zip(np.argsort(t)[:-num_top_words-1:-1],np.sort(t)[:-num_top_words-1:-1])]
  topic_words_values = ([top_words_values(t) for t in a])
  return [' '.join(t) for t in topic_words_values]

"""Mediante la función show_topics se muestran los primeros 8 tópicos identificados. A partir de estos tópicos se pueden obtener las siguientes conclusiones.

- Existen palabras que se repiten en distintos temas y que no representan ninguna derivación, como "consulta", "receta", "especificado" o "trastorno". 
- Se observan temas que sí pueden ser identificados como una derivación. Por ejemplo, es el caso del quinto y sexto tema. El quinto se puede categorizar como Endodoncia con palabras como "pulpitis", "periodontitis", "apical". El sexto tema se identifica como Medicina Interna, teniendo palabras como "mellitus", "insulinodependiente", "diabetes". El primer tema se podría catalogar como Ginecología y el cuarto como Traumatología, aunque no se puede decir tan claramente por la alta cantidad de palabras comunes. 
- Existen temas que son combinaciones de dos especialidades de derivación. Es el caso del tema 2 (combinación Ginecología y Trumatología), tema 3 (Oftalmología y Traumatología), tema 7 (Oftalmología, Endodoncia y Otorrinolaringología) y tema 8 (Oftalmología y Endodoncia).
- Este experimento evidencia la necesidad de utilizar "stop words médicas", específicas a este conjunto de datos. 

"""

# palabras y valores asociados a los tópicos
show_topics(Vh)[:8]

"""## 2.4 Visualización alternativa: WordCloud con TF-IDF y Cruz Roja

Para realizar una visualización de nube de palabras con tamaño más significativo para palabras que sean más específicas a las categorías de las prestacione, se puede graficar considerando TF-IDF. En esta WordCloud se observa que existen palabras representativas de las especialidades con un mayor tamaño, mejorando la visualización realizada por conteo de palabras.
"""

def plot_wordcloud_from_specialty_tfidf(specialty):
  tfidf_dict = dict(zip(tfidf_vectorizer_D10K.get_feature_names_out(),x_D10K.toarray()[D10K.PRESTA_EST == specialty].mean(0).reshape(-1,)))
  tfidf_dict = {word:val for word,val in tfidf_dict.items() if val > 0}
  wc = wordcloud.WordCloud(width=1600, height=800).generate_from_frequencies(tfidf_dict)
  plt.figure(figsize=(10,5))
  plt.imshow(wc)
  plt.axis("off")
  plt.show()

for specialty in list(D10K_rank.PRESTA_EST):
  print(f"{specialty}")
  plot_wordcloud_from_specialty_tfidf(specialty)
  print(f"")

"""Se incluye la visualización con el símbolo de la Cruz Roja."""

import PIL

def plot_redcross_from_specialty(specialty):
  tfidf_dict = dict(zip(tfidf_vectorizer_D10K.get_feature_names_out(),x_D10K.toarray()[D10K.PRESTA_EST == specialty].mean(0).reshape(-1,)))
  tfidf_dict = {word:val for word,val in tfidf_dict.items() if val > 0}
  mask = np.array(PIL.Image.open("swiss-cross-2-plus-cross-pattern-minimal-geometric-pattern-saltire-black-studio-grafiikka.jpg"))
  wc = wordcloud.WordCloud(width=800, height=800, mask=mask, background_color="white",color_func=lambda *args, **kwargs: "red").generate_from_frequencies(tfidf_dict)
  plt.figure(figsize=(10,5))
  plt.imshow(wc)
  plt.axis("off")
  plt.show()

for specialty in list(D10K_rank.PRESTA_EST):
  print(f"{specialty}")
  # plot_redcross_from_specialty(specialty)
  print(f"")

"""*   Liberar memoria que no se va a utilizar:"""

U = None
s = None
Vh = None
gc.collect()

"""# 3. Análisis supervisado

## 3.1 Probabilidad de cada categoría en D10K

Para calcular la probabilidad de cada categoría en D10K, primero se obtiene el total de categorías para luego dividir la cuenta de categoría por el total. Se asigna una nueva columna en el dataframe para la probabilidad. Es posible observar que las probabilidades están bien distribuidas entre clases.
"""

total_cat = D10K_rank.PRESTA_EST_COUNT.sum()
D10K_rank['PROB'] = D10K_rank.PRESTA_EST_COUNT/total_cat
D10K_rank

"""## 3.2 Diccionario P(palabra|categoria).

En esta sección se construye un diccionario, donde para cada palabra se obtiene la probabilidad P(palabra|categoria).
[IMAGEN]

Primero dividiremos la data de D10K en entrenamiento y test (90% y 10% respectivamente).
"""

train, test = train_test_split(D10K, test_size=0.1)

"""A continuación, se construye el vector en base a los datos de entrenamiento quitando los stop_words."""

count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(stop_words=stop_words)
term_doc_matrix_train = count_vectorizer.fit_transform(train.SOSPECHA_DIAG)
term_doc_matrix_train

"""Luego, se calcula la probabilidad de cada palabra por categoría."""

p_w_category = {}
p_w_s_category = {}
for index, row in D10K_rank.iterrows():
  category = row.PRESTA_EST
  count_c = np.asarray(term_doc_matrix_train.todense()[train.PRESTA_EST == category,:].sum(0)).reshape(-1).sum()
  # Se obtienen las apariciones de cada una de las palabras en cada clase
  c_w = np.asarray(term_doc_matrix_train.todense()[train.PRESTA_EST == category,:].sum(0)).reshape(-1)
  # Se calcula la probabilidad normal
  p_w = c_w / count_c
  # Se calcula la probabilidad suavizada para bayes
  vocab = len(np.asarray(term_doc_matrix_train.todense()[train.PRESTA_EST == category,:].sum(0)).reshape(-1))
  p_w_smoothing = (c_w + 1) / (count_c + vocab)
  
  p_w_category[category] = dict(zip(count_vectorizer.get_feature_names_out(), p_w))
  p_w_s_category[category] = dict(zip(count_vectorizer.get_feature_names_out(), p_w_smoothing))

"""En la siguiente celda se muestran las primeras 10 palabras con mayor probabilidad de cada clase del D10K."""

for category in p_w_category:
  print(category) 
  prob = p_w_category[category]
  better_prob = {k: prob[k] for k in sorted(prob, key=prob.get, reverse=True)[:10]}
  print(json.dumps(better_prob,indent=4))

"""Se observa que entre las palabras con mayor probabilidad de cada categoría, se encuentran palabras que sí tienen sentido, como: 


*   TRAUMATOLOGIA: artrosis, fractura, rodilla.
*   ENDODONCIA: pulpitis, pulpa, absceso.
*   CIRUGIA ADULTO: hernia, tumor.

Pero también hay palabras genéricas que están presentes y con alta probabilidad en todas las clases, como: 


*   especificada.
*   especificado.
*   consulta.

Se sugiere que para mejorar los resultados estas palabras sean quitadas de la data.

## 3.3 Clasificador Bayesiano Naive. 

Reporte el resultado de 3 métricas de clasificación ¿Detecto alguna categoría de mayor dificultad? ¿puede sugerir alguna interpretación a su resultado?

Se define un clasificador bayesiano en la función binary_naive_bayes donde se entrena, se prueba en el conjunto de test y se obtienen las métricas de clasificación: accuracy, precision y recall.
"""

for name, values in train.iteritems():
  if name not in ['PRESTA_EST', 'SOSPECHA_DIAG']:
    train.drop(name, axis=1, inplace=True)

for name, values in test.iteritems():
  if name not in ['PRESTA_EST', 'SOSPECHA_DIAG']:
    test.drop(name, axis=1, inplace=True)

vocab = len(np.asarray(term_doc_matrix_train.todense().sum(0)).reshape(-1))

vocab

def binary_naive_bayes(count_vectorizer, train_matrix, specialty):
  c_w_negative = np.asarray(train_matrix.todense()[train.PRESTA_EST != specialty, :].sum(0)).reshape(-1)
  c_w_positive = np.asarray(train_matrix.todense()[train.PRESTA_EST == specialty, :].sum(0)).reshape(-1)
  p_w_negative = (c_w_negative + 1) / (c_w_negative+vocab) # Probabilidad asociada a cada palabra en la clase negativa
  p_w_positive = (c_w_positive + 1) / (c_w_positive+vocab)
  r = np.log(p_w_positive / p_w_negative)
  term_doc_matrix_test = count_vectorizer.transform(test.SOSPECHA_DIAG)
  test_pred = term_doc_matrix_test.dot(r) > 0
  test_label = np.where(test.PRESTA_EST == specialty, True, False)
  confusion_matrix = pd.crosstab(test_label,test_pred)
  TP = confusion_matrix[1][1]
  FP = confusion_matrix[1][0]
  FN = confusion_matrix[0][1]
  accuracy = (np.array(confusion_matrix).diagonal().sum() / np.array(confusion_matrix).sum())
  precision = TP/(TP+FP)
  recall = TP/(TP+FN)
  return accuracy, precision, recall

"""Se obtienen las métricas de clasificación con el clasificador Bayesiano Naive para cada una de las especialidades. """

table_bayes_no_bigrams = []
for specialty in list(D10K_rank.PRESTA_EST):
  accuracy, precision, recall = binary_naive_bayes(count_vectorizer, term_doc_matrix_train, specialty)
  # print(f"specialty: {specialty}: accuracy={accuracy}, precision={precision}, recall={recall}")
  table_bayes_no_bigrams.append([specialty, str(round(accuracy*100.0, 2)), str(round(precision*100.0, 2)), str(round(recall*100.0, 2))])

"""En la tabla que se muestra a continuación se observa el accuracy, precision y recall para cada una de las especialidades. 

Las especialidades con términos más específicos son más fáciles de identificar por el clasificador bayesiano, por ejemplo oftalmología, otorrinolaringología y endodoncia. Por otro lado, las especialidades más generales, como cirugía adulto, medicina interna o ginecología presentan mayores dificultades para tener una buena clasificación. Esto se debe a que estas especialidades tratan problemas más generales de las personas. También es posible de observar este problema con las tres especialidades mencionadas ya que el Recall de estas categorías es el mayor, pese a tener un bajo accuracy, lo que quiere decir que hay una baja cantidad de falsos negativos.
"""

pd.DataFrame(table_bayes_no_bigrams, columns=['Especialidad', 'Accuracy (%)', 'Precision (%)', 'Recall (%)'])

"""*   Liberar memoria que no se va a utilizar:"""

count_vectorizer = None
term_doc_matrix_train = None
gc.collect()

"""## 3.4 Bigramas

En la presente sección se definen los bigramas y se hace una comparación con las métricas obtenidas con el clasificador bayesiano naive.
"""

count_vectorizer_bigrams = sklearn.feature_extraction.text.CountVectorizer(stop_words=stop_words, ngram_range=(1, 2))
term_doc_matrix_train_bigrams = count_vectorizer_bigrams.fit_transform(train.SOSPECHA_DIAG)

# ~2 minutos
table_bayes_bigrams = []
for idx, specialty in enumerate(list(D10K_rank.PRESTA_EST)):
  accuracy, precision, recall=binary_naive_bayes(count_vectorizer_bigrams, term_doc_matrix_train_bigrams, specialty)
  # print(f"specialty (with bigrams): {specialty}: accuracy={accuracy}, precision={precision}, recall={recall}")
  table_bayes_bigrams.append([specialty, table_bayes_no_bigrams[idx][1], str(round(accuracy*100.0, 2)), table_bayes_no_bigrams[idx][2], str(round(precision*100.0, 2)), table_bayes_no_bigrams[idx][3], str(round(recall*100.0, 2))])

"""La comparación de los dos resultados muestra que las métricas no mejoran significativamente con la utilización de bigramas. El clasificador bayesiano demuestra un rendimiento similar a los bigramas en todas las métricas. """

pd.DataFrame(table_bayes_bigrams, columns=['Especialidad', 'Accuracy (%)', 'Accuracy con bigramas (%)', 'Precision (%)', 'Precision con bigramas (%)', 'Recall (%)', 'Recall con bigramas (%)'])

"""# Conclusiones

Se concluye que el trabajo descrito cumplió con los objetivos planteados inicialmente, logrando visualizar las palabras más comunes de cada categoría para luego ser respaldados con el cálculo de probabilidades de cada palabra donde se pudo observar que los resultados coinciden.

La limpieza de la base de datos nos permitió obtener resultados más significativos para diferenciar las características entre las categorías, es parte fundamental del proceso. Por otro lado, se pudo observar que quedaron palabras del contexto de diagnostico médico que no generan características de valor en las categorías, como por ejemplo: consulta, diagnostico, especificado, entre otros. Ya que son palabras en común en todos los diagnosticos, es por esto que se sugiere agregar estas palabras a la limpieza de datos para obtener mejores resultados.

La representación de palabras en vectores densos, nos pemitió extraer las características entre las palabras y las oraciones los textos de sospecha diagnóstica. Luego, gracias al utilizar TF-IDF fue posible identificar que las palabras que ocurren frecuentemente son más importantes que las palabras infrecuentes, pero palabras que son demasiado frecuentes son poco importantes.

En el análisis no supervisado se obtuvieron las siguientes métricas "AQUI METRICAS" por lo que se concluye que el modelo tuvo un buen rendimiento y es capaz de predecir las derivaciones de los médicos en base a la sospecha diagnóstica.
"""